#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ULTRA PREDICTOR v2.0 - H·ªÜ TH·ªêNG D·ª∞ ƒêO√ÅN SI√äU C·∫§P
=====================================================
Chi·∫øn l∆∞·ª£c: Multi-Signal Ensemble + Combinatorial Coverage Optimization

C·∫£i ti·∫øn so v·ªõi phi√™n b·∫£n c≈©:
1. Multi-Signal Scoring: 7 t√≠n hi·ªáu kh√°c nhau ƒë·ªÉ ch·∫•m ƒëi·ªÉm m·ªói s·ªë
2. Ensemble AI: K·∫øt h·ª£p LSTM + Transformer + GRU + Statistical
3. Hot Zone Detection: Ph√°t hi·ªán 12-18 s·ªë "n√≥ng" nh·∫•t
4. Pair Co-occurrence: Ph√¢n t√≠ch c·∫∑p s·ªë hay xu·∫•t hi·ªán c√πng nhau
5. Cycle Detection: Ph√°t hi·ªán chu k·ª≥ xu·∫•t hi·ªán c·ªßa t·ª´ng s·ªë
6. Smart Ticket Generation: T·ªëi ∆∞u h√≥a ph·ªß s√≥ng gi·ªØa 10 v√©
7. Advanced Filters: L·ªçc th√¥ng minh theo quy lu·∫≠t th·ªëng k√™

M·ª•c ti√™u: N√¢ng t·ª´ 3-4 s·ªë tr√∫ng l√™n 5-6 s·ªë tr√∫ng
"""

import numpy as np
import pandas as pd
from collections import Counter, defaultdict
from itertools import combinations
from typing import List, Dict, Tuple, Optional
import json
import os
import sys
from datetime import datetime
from loguru import logger

# Suppress TF logs
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

try:
    from vietlott.config.products import get_config
except ImportError:
    class DummyConfig:
        def __init__(self, name):
            p1 = f"data/{name}.jsonl"
            p2 = f"data/{name.replace('_', '')}.jsonl"
            self.raw_path = p1 if os.path.exists(p1) else p2
    def get_config(name):
        return DummyConfig(name)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 1. NUMBER SCORER - Ch·∫•m ƒëi·ªÉm m·ªói s·ªë b·∫±ng nhi·ªÅu t√≠n hi·ªáu
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class NumberScorer:
    """Ch·∫•m ƒëi·ªÉm m·ªói s·ªë t·ª´ 1 ƒë·∫øn max_num b·∫±ng 7 t√≠n hi·ªáu kh√°c nhau."""
    
    def __init__(self, draws: List[List[int]], max_num: int = 45):
        self.draws = draws  # List of sorted number lists
        self.max_num = max_num
        self.num_draws = len(draws)
        self.scores = {}  # {number: {signal_name: score}}
        
    def compute_all_signals(self) -> Dict[int, float]:
        """T√≠nh t·∫•t c·∫£ t√≠n hi·ªáu v√† tr·∫£ v·ªÅ ƒëi·ªÉm t·ªïng h·ª£p cho m·ªói s·ªë."""
        print("   üìä T√≠nh to√°n 9 t√≠n hi·ªáu d·ª± ƒëo√°n (ƒë√£ t√≠ch h·ª£p RE Bias)...")
        
        s1 = self._frequency_score()
        s2 = self._recency_score()
        s3 = self._gap_analysis_score()
        s4 = self._cycle_detection_score()
        s5 = self._position_bias_score()
        s6 = self._momentum_score()
        s7 = self._co_occurrence_boost()
        s8 = self._streak_score()
        s9 = self._machine_bias_score() # T√≠n hi·ªáu 9: RE Bias
        
        # Tr·ªçng s·ªë cho m·ªói t√≠n hi·ªáu (v3.0 - Hybrid RE)
        weights = {
            'frequency': 0.05,
            'recency': 0.20,       # Gi·∫£m nh·∫π ƒë·ªÉ nh∆∞·ªùng ch·ªó cho bias
            'gap': 0.15,
            'cycle': 0.05,
            'position': 0.05,
            'momentum': 0.15,
            'cooccurrence': 0.05,
            'streak': 0.10,
            'machine_bias': 0.20,  # TR·ªåNG S·ªê CAO: Bias m√°y quay th·ª±c t·∫ø
        }
        
        final_scores = {}
        for num in range(1, self.max_num + 1):
            score = (
                weights['frequency'] * s1.get(num, 0) +
                weights['recency'] * s2.get(num, 0) +
                weights['gap'] * s3.get(num, 0) +
                weights['cycle'] * s4.get(num, 0) +
                weights['position'] * s5.get(num, 0) +
                weights['momentum'] * s6.get(num, 0) +
                weights['cooccurrence'] * s7.get(num, 0) +
                weights['streak'] * s8.get(num, 0) +
                weights['machine_bias'] * s9.get(num, 0)
            )
            
            # HARD FILTER: Dead numbers (Bias < 0) -> Score = -100
            if s9.get(num, 0) < 0:
                score = -100.0
                
            final_scores[num] = score
            
            
        # Normalize to [0, 1] (Handling Dead Numbers)
        valid_scores = [v for v in final_scores.values() if v > -50]
        max_s = max(valid_scores) if valid_scores else 1
        min_s = min(valid_scores) if valid_scores else 0
        range_s = max_s - min_s if max_s != min_s else 1
        
        for num in final_scores:
            if final_scores[num] <= -50:
                final_scores[num] = 0.0  # DEAD NUMBER -> 0 probability
            else:
                val = (final_scores[num] - min_s) / range_s
                final_scores[num] = max(0.001, val) # Ensure at least tiny probability for valid nums
                
        return final_scores
    
    def _frequency_score(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 1: T·∫ßn su·∫•t xu·∫•t hi·ªán t·ªïng th·ªÉ."""
        counter = Counter()
        for draw in self.draws:
            counter.update(draw)
        
        max_freq = max(counter.values()) if counter else 1
        return {num: counter.get(num, 0) / max_freq for num in range(1, self.max_num + 1)}
    
    def _recency_score(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 2: T·∫ßn su·∫•t g·∫ßn ƒë√¢y v·ªõi time-decay m·∫°nh (exponential)."""
        recent_window = min(20, len(self.draws))  # Gi·∫£m window xu·ªëng 20
        recent_draws = self.draws[-recent_window:]
        
        scores = defaultdict(float)
        for i, draw in enumerate(recent_draws):
            # Decay m·∫°nh h∆°n: 5 k·ª≥ g·∫ßn nh·∫•t chi·∫øm ph·∫ßn l·ªõn
            weight = np.exp(-0.15 * (recent_window - 1 - i))
            for num in draw:
                scores[num] += weight
        
        max_s = max(scores.values()) if scores else 1
        return {num: scores.get(num, 0) / max_s for num in range(1, self.max_num + 1)}
    
    def _gap_analysis_score(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 3: Kho·∫£ng c√°ch t·ª´ l·∫ßn xu·∫•t hi·ªán cu·ªëi (s·ªë "qu√° h·∫°n")."""
        last_seen = {}
        for i, draw in enumerate(self.draws):
            for num in draw:
                last_seen[num] = i
        
        # T√≠nh gap trung b√¨nh cho m·ªói s·ªë
        avg_gaps = {}
        for num in range(1, self.max_num + 1):
            appearances = [i for i, draw in enumerate(self.draws) if num in draw]
            if len(appearances) >= 2:
                gaps = [appearances[j+1] - appearances[j] for j in range(len(appearances)-1)]
                avg_gaps[num] = np.mean(gaps)
            else:
                avg_gaps[num] = len(self.draws)  # R·∫•t hi·∫øm xu·∫•t hi·ªán
        
        scores = {}
        for num in range(1, self.max_num + 1):
            current_gap = self.num_draws - 1 - last_seen.get(num, 0)
            avg_gap = avg_gaps.get(num, self.num_draws)
            
            if avg_gap > 0:
                # N·∫øu current_gap > avg_gap ‚Üí s·ªë n√†y "qu√° h·∫°n" ‚Üí ƒëi·ªÉm cao
                ratio = current_gap / avg_gap
                # Sigmoid-like function: cao nh·∫•t khi ratio ‚âà 1.0-1.5
                scores[num] = 1.0 / (1.0 + np.exp(-2 * (ratio - 1.0)))
            else:
                scores[num] = 0.5
        
        max_s = max(scores.values()) if scores else 1
        return {num: scores[num] / max_s for num in scores}
    
    def _cycle_detection_score(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 4: Ph√°t hi·ªán chu k·ª≥ xu·∫•t hi·ªán."""
        scores = {}
        for num in range(1, self.max_num + 1):
            appearances = [i for i, draw in enumerate(self.draws) if num in draw]
            
            if len(appearances) < 3:
                scores[num] = 0.3
                continue
            
            # T√≠nh c√°c kho·∫£ng c√°ch gi·ªØa c√°c l·∫ßn xu·∫•t hi·ªán
            gaps = [appearances[j+1] - appearances[j] for j in range(len(appearances)-1)]
            
            if len(gaps) < 2:
                scores[num] = 0.3
                continue
            
            # T√¨m chu k·ª≥ ph·ªï bi·∫øn nh·∫•t
            gap_counter = Counter(gaps)
            most_common_gap, common_count = gap_counter.most_common(1)[0]
            
            # Ki·ªÉm tra xem s·ªë n√†y c√≥ "ƒë·∫øn l∆∞·ª£t" theo chu k·ª≥ kh√¥ng
            current_gap = self.num_draws - 1 - appearances[-1]
            
            # N·∫øu current_gap g·∫ßn b·∫±ng most_common_gap ‚Üí ƒëi·ªÉm cao
            cycle_score = np.exp(-abs(current_gap - most_common_gap) / max(most_common_gap, 1))
            
            # Bonus n·∫øu chu k·ª≥ ·ªïn ƒë·ªãnh (variance th·∫•p)
            stability = common_count / len(gaps)
            
            scores[num] = cycle_score * (0.5 + 0.5 * stability)
        
        max_s = max(scores.values()) if scores else 1
        return {num: scores[num] / max_s for num in scores}
    
    def _position_bias_score(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 5: Thi√™n v·ªã theo v·ªã tr√≠ (s·ªë th·ª© 1-6 trong b·ªô)."""
        # M·ªôt s·ªë s·ªë c√≥ xu h∆∞·ªõng xu·∫•t hi·ªán ·ªü v·ªã tr√≠ nh·∫•t ƒë·ªãnh
        position_counts = defaultdict(lambda: defaultdict(int))
        
        for draw in self.draws:
            sorted_draw = sorted(draw[:6])  # Ch·ªâ l·∫•y 6 s·ªë ch√≠nh
            for pos, num in enumerate(sorted_draw):
                position_counts[num][pos] += 1
        
        scores = {}
        for num in range(1, self.max_num + 1):
            if num in position_counts:
                total = sum(position_counts[num].values())
                # S·ªë c√≥ xu h∆∞·ªõng v·ªã tr√≠ r√µ r√†ng ‚Üí ƒëi·ªÉm cao h∆°n
                max_pos_count = max(position_counts[num].values())
                scores[num] = max_pos_count / max(total, 1)
            else:
                scores[num] = 0.0
        
        max_s = max(scores.values()) if scores else 1
        return {num: scores[num] / max_s for num in scores}
    
    def _momentum_score(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 6: ƒê√† tƒÉng/gi·∫£m (so s√°nh t·∫ßn su·∫•t g·∫ßn ƒë√¢y vs. t·ªïng th·ªÉ)."""
        # T·∫ßn su·∫•t t·ªïng th·ªÉ
        overall_freq = Counter()
        for draw in self.draws:
            overall_freq.update(draw)
        
        # T·∫ßn su·∫•t 10 k·ª≥ g·∫ßn nh·∫•t (ng·∫Øn h∆°n ƒë·ªÉ b·∫Øt trend nhanh)
        recent_window = min(10, len(self.draws))
        recent_freq = Counter()
        for draw in self.draws[-recent_window:]:
            recent_freq.update(draw)
        
        scores = {}
        for num in range(1, self.max_num + 1):
            overall_rate = overall_freq.get(num, 0) / max(self.num_draws, 1)
            recent_rate = recent_freq.get(num, 0) / max(recent_window, 1)
            
            # Momentum = recent_rate / overall_rate
            if overall_rate > 0:
                momentum = recent_rate / overall_rate
                # S·ªë ƒëang "n√≥ng" (momentum > 1) ‚Üí ƒëi·ªÉm cao
                scores[num] = min(momentum, 4.0) / 4.0  # M·ªü r·ªông thang
            else:
                scores[num] = 0.0
        
        max_s = max(scores.values()) if scores else 1
        return {num: scores[num] / max_s for num in scores}
    
    def _co_occurrence_boost(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 7: Hay ƒëi c√πng v·ªõi c√°c s·ªë kh√°c (pair analysis - g·∫ßn ƒë√¢y)."""
        # Ch·ªâ d√πng 40 k·ª≥ g·∫ßn nh·∫•t cho pair analysis (focus recent)
        recent_draws = self.draws[-min(40, len(self.draws)):]
        pair_counts = Counter()
        for i, draw in enumerate(recent_draws):
            weight = 1.0 + 0.5 * (i / len(recent_draws))  # Recent pairs weighted more
            nums = sorted(draw[:6])
            for pair in combinations(nums, 2):
                pair_counts[pair] += weight
        
        # Cho m·ªói s·ªë, t√≠nh "s·ª©c m·∫°nh li√™n k·∫øt" trung b√¨nh
        scores = {}
        for num in range(1, self.max_num + 1):
            relevant_pairs = [(p, c) for p, c in pair_counts.items() if num in p]
            if relevant_pairs:
                avg_pair_strength = np.mean([c for _, c in relevant_pairs])
                scores[num] = avg_pair_strength
            else:
                scores[num] = 0.0
        
        max_s = max(scores.values()) if scores else 1
        return {num: scores[num] / max_s for num in scores}
    
    def _streak_score(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 8 (M·ªöI): Ph√°t hi·ªán s·ªë xu·∫•t hi·ªán li√™n t·ª•c g·∫ßn ƒë√¢y."""
        scores = {}
        window = min(8, len(self.draws))  # Check 8 k·ª≥ g·∫ßn nh·∫•t
        recent = self.draws[-window:]
        
        for num in range(1, self.max_num + 1):
            # ƒê·∫øm trong bao nhi√™u k·ª≥ g·∫ßn ƒë√¢y s·ªë n√†y xu·∫•t hi·ªán
            appearances = sum(1 for draw in recent if num in draw)
            
            # Streak bonus: xu·∫•t hi·ªán 2+ l·∫ßn trong 8 k·ª≥ g·∫ßn nh·∫•t
            if appearances >= 4:
                scores[num] = 1.0  # Xu·∫•t hi·ªán 4+/8 k·ª≥ ‚Üí si√™u n√≥ng
            elif appearances >= 3:
                scores[num] = 0.85
            elif appearances >= 2:
                scores[num] = 0.6
            elif appearances >= 1:
                scores[num] = 0.3
            else:
                scores[num] = 0.0
            
            # Extra bonus: xu·∫•t hi·ªán trong k·ª≥ G·∫¶N NH·∫§T
            if num in recent[-1]:
                scores[num] = min(1.0, scores[num] + 0.15)
            # Extra bonus: xu·∫•t hi·ªán trong 2 k·ª≥ g·∫ßn nh·∫•t li√™n ti·∫øp
            if len(recent) >= 2 and num in recent[-1] and num in recent[-2]:
                scores[num] = min(1.0, scores[num] + 0.10)
        
        max_s = max(scores.values()) if scores else 1
        return {num: scores[num] / max_s for num in scores}

    def _machine_bias_score(self) -> Dict[int, float]:
        """T√≠n hi·ªáu 9 (M·ªöI - QUAN TR·ªåNG): Bias m√°y quay t·ª´ Reverse Engineering."""
        scores = {}
        
        # POWER 6/55 BIAS
        # Ph√°t hi·ªán t·ª´ RE: S·ªë > 50 c·ª±c hi·∫øm, ƒë·∫∑c bi·ªát 53-55 l√† "s·ªë ch·∫øt"
        if self.max_num == 55:
            # 1. Dead Zone (V√πng ch·∫øt) - Ph·∫°t c·ª±c n·∫∑ng
            for num in range(1, self.max_num + 1):
                if num in [55, 54]:      # T·ª≠ huy·ªát: Ch∆∞a bao gi·ªù ra ho·∫∑c c·ª±c hi·∫øm
                    scores[num] = -1.0   # Score √¢m ƒë·ªÉ ch·∫Øc ch·∫Øn kh√¥ng ƒë∆∞·ª£c ch·ªçn
                elif num in [53, 52]:    # R·∫•t l·∫°nh
                    scores[num] = 0.1
                elif num in [50, 51, 48, 49]: # L·∫°nh
                    scores[num] = 0.2
                elif num in [45, 46, 47]:     # H∆°i l·∫°nh
                    scores[num] = 0.3
                else:
                    scores[num] = 0.5    # Neutral baseline
            
            # 2. Hot Bias (V√πng n√≥ng) - Boost s·ªë m√°y quay "th√≠ch"
            hot_bias_nums = [22, 34, 9, 20, 8, 23, 3, 31, 1, 12]
            for num in hot_bias_nums:
                scores[num] = min(1.0, scores[num] + 0.4)
                
            # 3. Pair Bias Boost - C·∫∑p s·ªë hay ƒëi c√πng nhau (t·ª´ RE)
            pair_boost = {
                13: 0.2, 9: 0.1,  # Pair (9, 13)
                11: 0.2, 22: 0.1, # Pair (11, 22)
            }
            for num, boost in pair_boost.items():
                if scores[num] > 0: # Ch·ªâ boost n·∫øu kh√¥ng ph·∫£i s·ªë ch·∫øt
                    scores[num] = min(1.0, scores[num] + boost)
                
        # MEGA 6/45 FAIRNESS
        # Ph√°t hi·ªán t·ª´ RE: M√°y quay Mega r·∫•t c√¥ng b·∫±ng, kh√¥ng c√≥ dead zone
        else:
            for num in range(1, self.max_num + 1):
                scores[num] = 0.5  # Neutral baseline
            
            # Ch·ªâ boost nh·∫π c√°c s·ªë hay ra v√¨ bias v·ªã tr√≠ (t·ª± nhi√™n)
            pos_bias_nums = [1, 2, 4, 3, 5]
            for num in pos_bias_nums:
                scores[num] = min(1.0, scores[num] + 0.1)
            
            # C·∫∑p s·ªë hay ƒëi c√πng (t·ª´ RE)
            pair_boost = {
                10: 0.1, 22: 0.1,
                18: 0.1, 29: 0.1,
                24: 0.1, 37: 0.1
            }
            for num, boost in pair_boost.items():
                scores[num] = min(1.0, scores[num] + boost)
                
        return scores
    
    def get_pair_matrix(self) -> Dict[Tuple[int, int], float]:
        """Tr·∫£ v·ªÅ ma tr·∫≠n co-occurrence cho t·∫•t c·∫£ c·∫∑p s·ªë (focus g·∫ßn ƒë√¢y)."""
        pair_counts = Counter()
        recent_draws = self.draws[-min(40, len(self.draws)):]  # Gi·∫£m xu·ªëng 40 k·ª≥
        for i, draw in enumerate(recent_draws):
            weight = 1.0 + (i / len(recent_draws))  # Recent weighted more
            nums = sorted(draw[:6])
            for pair in combinations(nums, 2):
                pair_counts[pair] += weight
        
        # Normalize
        max_c = max(pair_counts.values()) if pair_counts else 1
        return {pair: count / max_c for pair, count in pair_counts.items()}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 2. DEEP ENSEMBLE - K·∫øt h·ª£p nhi·ªÅu m√¥ h√¨nh AI
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class DeepEnsemble:
    """K·∫øt h·ª£p LSTM s√¢u + Transformer n√¢ng cao ƒë·ªÉ cho x√°c su·∫•t m·ªói s·ªë."""
    
    def __init__(self, draws: List[List[int]], max_num: int = 45):
        self.draws = draws
        self.max_num = max_num
        self.look_back = min(20, len(draws) - 1)  # C·ª≠a s·ªï l√πi 20 k·ª≥
        
    def _prepare_binary_data(self) -> np.ndarray:
        """Chuy·ªÉn draws th√†nh binary vectors."""
        vectors = []
        for draw in self.draws:
            vec = np.zeros(self.max_num)
            for num in draw:
                if 1 <= num <= self.max_num:
                    vec[num - 1] = 1
            vectors.append(vec)
        return np.array(vectors)
    
    def _create_sequences(self, data: np.ndarray):
        """T·∫°o sequences cho time series."""
        X, y = [], []
        for i in range(len(data) - self.look_back):
            X.append(data[i:(i + self.look_back)])
            y.append(data[i + self.look_back])
        return np.array(X), np.array(y)
    
    def run_enhanced_lstm(self, epochs: int = 50) -> Optional[np.ndarray]:
        """LSTM n√¢ng cao: Bi-directional + 3 t·∫ßng + Attention."""
        try:
            import tensorflow as tf
            from tensorflow.keras import layers, models, optimizers, callbacks
        except ImportError:
            print("   ‚ö†Ô∏è TensorFlow kh√¥ng kh·∫£ d·ª•ng, b·ªè qua LSTM.")
            return None
        
        print("   üß† Training Enhanced Bi-LSTM (3 t·∫ßng + Attention)...")
        
        data = self._prepare_binary_data()
        X, y = self._create_sequences(data)
        
        if len(X) < 10:
            print("   ‚ö†Ô∏è Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho LSTM.")
            return None
        
        # Split
        split = max(1, len(X) - 50)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        # Enhanced LSTM Architecture
        inputs = layers.Input(shape=(self.look_back, self.max_num))
        
        # Bidirectional LSTM layer 1
        x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # Bidirectional LSTM layer 2
        x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # Self-attention mechanism
        attention = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
        x = layers.Add()([x, attention])
        x = layers.LayerNormalization()(x)
        
        # LSTM layer 3  
        x = layers.LSTM(64)(x)
        x = layers.Dropout(0.3)(x)
        
        # Dense layers
        x = layers.Dense(128, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        x = layers.Dense(64, activation='relu')(x)
        outputs = layers.Dense(self.max_num, activation='sigmoid')(x)
        
        model = models.Model(inputs=inputs, outputs=outputs)
        model.compile(
            loss='binary_crossentropy',
            optimizer=optimizers.Adam(learning_rate=0.001),
            metrics=['accuracy']
        )
        
        # Training with callbacks
        early_stop = callbacks.EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True, verbose=0
        )
        reduce_lr = callbacks.ReduceLROnPlateau(
            monitor='val_loss', factor=0.5, patience=5, verbose=0
        )
        
        model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=epochs,
            batch_size=16,
            callbacks=[early_stop, reduce_lr],
            verbose=0
        )
        
        # Evaluate
        if len(X_test) > 0:
            preds = model.predict(X_test, verbose=0)
            avg_match = 0
            for i in range(len(preds)):
                top6 = np.argsort(preds[i])[-6:]
                actual = np.where(y_test[i] == 1)[0]
                avg_match += len(set(top6) & set(actual))
            avg_match /= len(preds)
            print(f"   üìà LSTM Test accuracy: {avg_match:.2f}/6 tr√πng kh·ªõp TB")
        
        # Predict next
        last_seq = data[-self.look_back:].reshape(1, self.look_back, self.max_num)
        probs = model.predict(last_seq, verbose=0)[0]
        
        return probs
    
    def run_deep_transformer(self, epochs: int = 50) -> Optional[np.ndarray]:
        """Transformer n√¢ng cao v·ªõi multiple attention layers."""
        try:
            import tensorflow as tf
            from tensorflow.keras import layers, models, optimizers, callbacks
        except ImportError:
            print("   ‚ö†Ô∏è TensorFlow kh√¥ng kh·∫£ d·ª•ng, b·ªè qua Transformer.")
            return None
        
        print("   ü§ñ Training Deep Transformer (3 blocks, 8 heads)...")
        
        data = self._prepare_binary_data()
        X, y = self._create_sequences(data)
        
        if len(X) < 10:
            return None
        
        split = max(1, len(X) - 50)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        # Transformer Architecture - 3 blocks
        inputs = layers.Input(shape=(self.look_back, self.max_num))
        
        # Positional encoding via dense projection
        x = layers.Dense(64)(inputs)
        
        # Transformer Block 1
        attn1 = layers.MultiHeadAttention(num_heads=8, key_dim=32)(x, x)
        attn1 = layers.Dropout(0.1)(attn1)
        x = layers.Add()([x, attn1])
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        ff1 = layers.Dense(128, activation='gelu')(x)
        ff1 = layers.Dense(64)(ff1)
        ff1 = layers.Dropout(0.1)(ff1)
        x = layers.Add()([x, ff1])
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        
        # Transformer Block 2
        attn2 = layers.MultiHeadAttention(num_heads=8, key_dim=32)(x, x)
        attn2 = layers.Dropout(0.1)(attn2)
        x = layers.Add()([x, attn2])
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        ff2 = layers.Dense(128, activation='gelu')(x)
        ff2 = layers.Dense(64)(ff2)
        ff2 = layers.Dropout(0.1)(ff2)
        x = layers.Add()([x, ff2])
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        
        # Transformer Block 3
        attn3 = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
        attn3 = layers.Dropout(0.1)(attn3)
        x = layers.Add()([x, attn3])
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        
        # Output
        x = layers.GlobalAveragePooling1D()(x)
        x = layers.Dense(128, activation='relu')(x)
        x = layers.Dropout(0.3)(x)
        x = layers.Dense(64, activation='relu')(x)
        outputs = layers.Dense(self.max_num, activation='sigmoid')(x)
        
        model = models.Model(inputs=inputs, outputs=outputs)
        model.compile(
            loss='binary_crossentropy',
            optimizer=optimizers.Adam(learning_rate=0.001),
            metrics=['accuracy']
        )
        
        early_stop = callbacks.EarlyStopping(
            monitor='val_loss', patience=10, restore_best_weights=True, verbose=0
        )
        reduce_lr = callbacks.ReduceLROnPlateau(
            monitor='val_loss', factor=0.5, patience=5, verbose=0
        )
        
        model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=epochs,
            batch_size=16,
            callbacks=[early_stop, reduce_lr],
            verbose=0
        )
        
        # Evaluate
        if len(X_test) > 0:
            preds = model.predict(X_test, verbose=0)
            avg_match = 0
            for i in range(len(preds)):
                top6 = np.argsort(preds[i])[-6:]
                actual = np.where(y_test[i] == 1)[0]
                avg_match += len(set(top6) & set(actual))
            avg_match /= len(preds)
            print(f"   üìà Transformer Test accuracy: {avg_match:.2f}/6 tr√πng kh·ªõp TB")
        
        # Predict next
        last_seq = data[-self.look_back:].reshape(1, self.look_back, self.max_num)
        probs = model.predict(last_seq, verbose=0)[0]
        
        return probs
    
    def run_gru_model(self, epochs: int = 40) -> Optional[np.ndarray]:
        """GRU model - nhanh h∆°n LSTM, ƒë√¥i khi t·ªët h∆°n."""
        try:
            import tensorflow as tf
            from tensorflow.keras import layers, models, optimizers, callbacks
        except ImportError:
            return None
        
        print("   ‚ö° Training GRU Model (2 t·∫ßng)...")
        
        data = self._prepare_binary_data()
        X, y = self._create_sequences(data)
        
        if len(X) < 10:
            return None
        
        split = max(1, len(X) - 50)
        X_train, X_test = X[:split], X[split:]
        y_train, y_test = y[:split], y[split:]
        
        inputs = layers.Input(shape=(self.look_back, self.max_num))
        x = layers.GRU(128, return_sequences=True)(inputs)
        x = layers.Dropout(0.2)(x)
        x = layers.GRU(64)(x)
        x = layers.Dropout(0.2)(x)
        x = layers.Dense(128, activation='relu')(x)
        x = layers.Dropout(0.3)(x)
        outputs = layers.Dense(self.max_num, activation='sigmoid')(x)
        
        model = models.Model(inputs=inputs, outputs=outputs)
        model.compile(loss='binary_crossentropy', optimizer='adam')
        
        early_stop = callbacks.EarlyStopping(
            monitor='val_loss', patience=8, restore_best_weights=True, verbose=0
        )
        
        model.fit(X_train, y_train, validation_data=(X_test, y_test),
                  epochs=epochs, batch_size=16, callbacks=[early_stop], verbose=0)
        
        last_seq = data[-self.look_back:].reshape(1, self.look_back, self.max_num)
        probs = model.predict(last_seq, verbose=0)[0]
        
        return probs
    
    def get_ensemble_probabilities(self) -> np.ndarray:
        """Ch·∫°y t·∫•t c·∫£ models v√† k·∫øt h·ª£p x√°c su·∫•t."""
        print("\n" + "üî•" * 30)
        print("KH·ªûI ƒê·ªòNG DEEP ENSEMBLE AI (3 Models)")
        print("üî•" * 30)
        
        all_probs = []
        weights = []
        
        # Model 1: Enhanced LSTM (tr·ªçng s·ªë cao nh·∫•t)
        lstm_probs = self.run_enhanced_lstm(epochs=50)
        if lstm_probs is not None:
            all_probs.append(lstm_probs)
            weights.append(0.40)
        
        # Model 2: Deep Transformer
        transformer_probs = self.run_deep_transformer(epochs=50)
        if transformer_probs is not None:
            all_probs.append(transformer_probs)
            weights.append(0.35)
        
        # Model 3: GRU
        gru_probs = self.run_gru_model(epochs=40)
        if gru_probs is not None:
            all_probs.append(gru_probs)
            weights.append(0.25)
        
        if not all_probs:
            print("   ‚ùå Kh√¥ng c√≥ model n√†o ch·∫°y ƒë∆∞·ª£c!")
            return np.ones(self.max_num) / self.max_num
        
        # Normalize weights
        total_w = sum(weights[:len(all_probs)])
        weights = [w / total_w for w in weights[:len(all_probs)]]
        
        # Weighted average
        ensemble = np.zeros(self.max_num)
        for probs, weight in zip(all_probs, weights):
            ensemble += weight * probs
        
        print(f"\n   ‚úÖ Ensemble ho√†n t·∫•t ({len(all_probs)} models)")
        top6 = np.argsort(ensemble)[-6:][::-1]
        top6_nums = sorted([i + 1 for i in top6])
        print(f"   üîÆ Top 6 t·ª´ AI: {top6_nums}")
        
        return ensemble


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 3. TICKET OPTIMIZER - T·ªëi ∆∞u h√≥a b·ªô v√©
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class TicketOptimizer:
    """T·∫°o b·ªô v√© t·ªëi ∆∞u b·∫±ng chi·∫øn l∆∞·ª£c MULTI-ZONE v3.0."""
    
    def __init__(self, max_num: int = 45, draws: List[List[int]] = None):
        self.max_num = max_num
        self.draws = draws or []
        
        # T√≠nh th·ªëng k√™ cho filters
        sums = [sum(d[:6]) for d in self.draws] if self.draws else []
        if sums:
            self.min_sum = np.percentile(sums, 5)
            self.max_sum = np.percentile(sums, 95)
            self.mean_sum = np.mean(sums)
        else:
            self.min_sum = 60
            self.max_sum = 200
            self.mean_sum = 130
    
    def _passes_filters(self, nums: List[int], strict: bool = True) -> bool:
        """Ki·ªÉm tra b·ªô s·ªë c√≥ h·ª£p l·ªá theo c√°c b·ªô l·ªçc th·ªëng k√™.
        strict=True: l·ªçc nghi√™m (cho core zone)
        strict=False: l·ªçc nh·∫π (cho extended/wild zone)
        """
        nums = sorted(nums)
        
        # Filter 1: T·ªïng trong kho·∫£ng h·ª£p l√Ω
        s = sum(nums)
        if strict:
            if not (self.min_sum <= s <= self.max_sum):
                return False
        else:
            # Relaxed: m·ªü r·ªông 15%
            margin = (self.max_sum - self.min_sum) * 0.15
            if not (self.min_sum - margin <= s <= self.max_sum + margin):
                return False
        
        # Filter 2: T·ª∑ l·ªá ch·∫µn/l·∫ª
        evens = sum(1 for n in nums if n % 2 == 0)
        if strict:
            if evens < 2 or evens > 4:
                return False
        else:
            if evens < 1 or evens > 5:
                return False
        
        # Filter 3: Kh√¥ng qu√° 2 s·ªë li√™n ti·∫øp (VD: 1,2,3 l√† x·∫•u)
        for i in range(len(nums) - 2):
            if nums[i+2] == nums[i+1] + 1 == nums[i] + 2:
                return False
        
        # Filter 4: √çt nh·∫•t 3 ƒëu√¥i s·ªë kh√°c nhau (relaxed from 4)
        last_digits = set(n % 10 for n in nums)
        min_digits = 4 if strict else 3
        if len(last_digits) < min_digits:
            return False
        
        # Filter 5: Tr·∫£i ƒë·ªÅu qua c√°c decade (√≠t nh·∫•t 3 decades)
        decades = set(n // 10 for n in nums)
        min_decades = 3 if strict else 2
        if len(decades) < min_decades:
            return False
        
        # Filter 6: Kho·∫£ng c√°ch l·ªõn nh·∫•t
        max_gap = max(nums[i+1] - nums[i] for i in range(len(nums) - 1))
        max_allowed = self.max_num * 0.55 if strict else self.max_num * 0.7
        if max_gap > max_allowed:
            return False
        
        return True
    
    def _score_ticket(self, ticket, combined_scores, pair_matrix) -> float:
        """Ch·∫•m ƒëi·ªÉm m·ªôt b·ªô v√©."""
        # ƒêi·ªÉm 1: T·ªïng ƒëi·ªÉm individual
        individual_score = sum(combined_scores.get(n, 0) for n in ticket)
        
        # ƒêi·ªÉm 2: Pair bonus
        pair_score = 0
        for pair in combinations(ticket, 2):
            pair_key = tuple(sorted(pair))
            pair_score += pair_matrix.get(pair_key, 0)
        
        # ƒêi·ªÉm 3: Sum proximity
        sum_diff = abs(sum(ticket) - self.mean_sum)
        sum_score = np.exp(-sum_diff / 50)
        
        return (
            0.45 * individual_score / 6 +
            0.35 * pair_score / 15 +
            0.20 * sum_score
        )
    
    def generate_optimal_tickets(
        self,
        number_scores: Dict[int, float],
        ai_probs: np.ndarray,
        pair_matrix: Dict[Tuple[int, int], float],
        count: int = 18
    ) -> List[List[int]]:
        """
        CHI·∫æN L∆Ø·ª¢C HYBRID v4.0 = Ultra (Hot Zone) + RE (Weighted Random):
        
        Zone A (CORE - 8 s·ªë): 8 v√© t·∫≠p trung nh·∫•t ‚Üí m·ª•c ti√™u 4-6 s·ªë tr√∫ng
        Zone B (EXTENDED - 14 s·ªë): 4 v√© ph·ªß r·ªông h∆°n ‚Üí m·ª•c ti√™u 3-5 s·ªë tr√∫ng  
        Zone C (RE-RANDOM - T·∫§T C·∫¢ s·ªë): 6 v√© random c√≥ tr·ªçng s·ªë ‚Üí b·∫Øt s·ªë b·∫•t ng·ªù
        
        T·ªïng: 18 v√© t·ªëi ∆∞u
        """
        print("\n" + "üèÜ" * 30)
        print("HYBRID OPTIMIZER v4.0 (Ultra + RE) - 18 V√â")
        print("üèÜ" * 30)
        
        # B∆Ø·ªöC 1: K·∫øt h·ª£p ƒëi·ªÉm statistical + AI
        combined_scores = {}
        for num in range(1, self.max_num + 1):
            stat_score = number_scores.get(num, 0)
            ai_score = ai_probs[num - 1] if num - 1 < len(ai_probs) else 0
            combined_scores[num] = 0.50 * stat_score + 0.50 * ai_score
        
        sorted_numbers = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        
        # B∆Ø·ªöC 2: T·∫°o zones
        core_size = 8 if self.max_num <= 45 else 9
        ext_size = 14 if self.max_num <= 45 else 16
        
        core_zone = [num for num, _ in sorted_numbers[:core_size]]
        ext_zone = [num for num, _ in sorted_numbers[:ext_size]]
        
        print(f"\n   üî¥ CORE ZONE  ({len(core_zone)} s·ªë): {sorted(core_zone)}")
        print(f"   üü° EXT ZONE   ({len(ext_zone)} s·ªë): {sorted(ext_zone)}")
        print(f"   ÔøΩ RE-RANDOM  (T·∫§T C·∫¢ {self.max_num} s·ªë, weighted sampling)")
        
        print(f"\n   üìä ƒêi·ªÉm Top {ext_size} s·ªë:")
        for num, score in sorted_numbers[:ext_size]:
            zone = "üî¥" if num in core_zone else "üü°"
            bar = "‚ñà" * int(score * 25)
            print(f"      {zone} S·ªë {num:2d}: {score:.3f} {bar}")
        
        # B∆Ø·ªöC 3: T·∫°o candidates cho m·ªói zone
        print(f"\n   ‚öôÔ∏è ƒêang t·∫°o t·ªï h·ª£p...")
        
        # Zone A: Core (brute force)
        core_candidates = list(combinations(core_zone, 6))
        core_scored = []
        for ticket in core_candidates:
            ticket = sorted(list(ticket))
            if self._passes_filters(ticket, strict=True):
                score = self._score_ticket(ticket, combined_scores, pair_matrix)
                core_scored.append((ticket, score))
        core_scored.sort(key=lambda x: x[1], reverse=True)
        
        # Zone B: Extended (brute force)
        ext_candidates = list(combinations(ext_zone, 6))
        ext_scored = []
        for ticket in ext_candidates:
            ticket = sorted(list(ticket))
            if self._passes_filters(ticket, strict=True):
                score = self._score_ticket(ticket, combined_scores, pair_matrix)
                ext_scored.append((ticket, score))
        core_set = set(tuple(t) for t, _ in core_scored)
        ext_scored = [(t, s) for t, s in ext_scored if tuple(t) not in core_set]
        ext_scored.sort(key=lambda x: x[1], reverse=True)
        
        # Zone C: RE-STYLE WEIGHTED RANDOM (Reverse Engineering approach)
        # Key: sample t·ª´ T·∫§T C·∫¢ s·ªë, kh√¥ng gi·ªõi h·∫°n hot zone
        re_scored = self._generate_re_tickets(combined_scores, pair_matrix, n_tickets=200)
        # Lo·∫°i tr√πng v·ªõi core/ext
        ext_set = set(tuple(t) for t, _ in ext_scored)
        re_scored = [(t, s) for t, s in re_scored if tuple(t) not in core_set and tuple(t) not in ext_set]
        re_scored.sort(key=lambda x: x[1], reverse=True)
        
        print(f"   ‚úÖ Core: {len(core_scored)} | Ext: {len(ext_scored)} | RE-Random: {len(re_scored)}")
        
        # B∆Ø·ªöC 4: Ph√¢n b·ªï v√©
        core_count = min(8, count * 8 // 18)   # ~44% cho core (t·∫≠p trung)
        ext_count = min(4, count * 4 // 18)    # ~22% cho extended
        re_count = count - core_count - ext_count  # ~33% cho RE-random (nhi·ªÅu h∆°n!)
        
        selected_core = self._select_with_coverage(core_scored, core_count)
        selected_ext = self._select_with_coverage(ext_scored, ext_count)
        selected_re = self._select_with_coverage(re_scored, re_count)
        
        all_selected = selected_core + selected_ext + selected_re
        
        # In k·∫øt qu·∫£
        print(f"\n   üéØ ƒê√É CH·ªåN {len(all_selected)} B·ªò S·ªê T·ªêI ∆ØU:")
        for i, ticket in enumerate(all_selected, 1):
            if i <= len(selected_core):
                zone_label = "üî¥ CORE"
            elif i <= len(selected_core) + len(selected_ext):
                zone_label = "üü° EXT "
            else:
                zone_label = "ÔøΩ RE  "
            score = self._score_ticket(ticket, combined_scores, pair_matrix)
            print(f"      {zone_label} {i:2d}. {ticket} (Œ£={sum(ticket)}, Score={score:.4f})")
        
        # Th·ªëng k√™ coverage
        all_nums = set()
        for t in all_selected:
            all_nums.update(t)
        
        core_nums = set()
        for t in selected_core:
            core_nums.update(t)
        re_only_nums = all_nums - set(ext_zone)
        
        print(f"\n   üìä Ph·ªß s√≥ng: {len(all_nums)} s·ªë t·ªïng c·ªông")
        print(f"   üìä C√°c s·ªë: {sorted(all_nums)}")
        if re_only_nums:
            print(f"   üü£ S·ªë t·ª´ RE (ngo√†i hot zone): {sorted(re_only_nums)}")
        
        return all_selected
    
    def _generate_re_tickets(
        self,
        combined_scores: Dict[int, float],
        pair_matrix: Dict,
        n_tickets: int = 200
    ) -> List[Tuple[List[int], float]]:
        """
        Reverse Engineering style: Weighted random sampling t·ª´ T·∫§T C·∫¢ s·ªë.
        D√πng combined_scores l√†m tr·ªçng s·ªë sampling.
        """
        # T·∫°o probability distribution t·ª´ scores
        all_nums = list(range(1, self.max_num + 1))
        raw_probs = np.array([combined_scores.get(n, 0) for n in all_nums])
        
        # Boost: n√¢ng min prob l√™n ƒë·ªÉ s·ªë √≠t ƒëi·ªÉm v·∫´n c√≥ c∆° h·ªôi
        raw_probs = raw_probs + 0.05  # Floor probability
        probs = raw_probs / raw_probs.sum()
        
        scored_tickets = []
        seen = set()
        attempts = 0
        max_attempts = n_tickets * 50
        
        while len(scored_tickets) < n_tickets and attempts < max_attempts:
            attempts += 1
            try:
                ticket = sorted(np.random.choice(all_nums, size=6, replace=False, p=probs).tolist())
            except Exception:
                ticket = sorted(np.random.choice(all_nums, size=6, replace=False).tolist())
            
            ticket_key = tuple(ticket)
            if ticket_key in seen:
                continue
            seen.add(ticket_key)
            
            if not self._passes_filters(ticket, strict=False):
                continue
            
            score = self._score_ticket(ticket, combined_scores, pair_matrix)
            scored_tickets.append((ticket, score))
        
        return scored_tickets
    
    def _smart_sample_tickets(
        self,
        hot_zone: List[int],
        scores: Dict[int, float],
        pair_matrix: Dict,
        n_samples: int
    ) -> List[Tuple[int, ...]]:
        """Sampling th√¥ng minh d·ª±a tr√™n x√°c su·∫•t."""
        probs = np.array([scores.get(n, 0) for n in hot_zone])
        probs_sum = probs.sum()
        if probs_sum > 0:
            probs = probs / probs_sum
        else:
            probs = np.ones(len(hot_zone)) / len(hot_zone)
        
        candidates = set()
        for _ in range(n_samples):
            try:
                ticket = tuple(sorted(np.random.choice(hot_zone, size=6, replace=False, p=probs)))
                candidates.add(ticket)
            except Exception:
                ticket = tuple(sorted(np.random.choice(hot_zone, size=6, replace=False)))
                candidates.add(ticket)
        
        return list(candidates)
    
    def _select_with_coverage(
        self,
        scored_candidates: List[Tuple[List[int], float]],
        count: int
    ) -> List[List[int]]:
        """Ch·ªçn tickets sao cho maximize coverage + score."""
        if not scored_candidates:
            return []
        
        selected = []
        covered_pairs = set()
        covered_numbers = set()
        
        for _ in range(count):
            best_ticket = None
            best_value = -1
            
            # Scan top 500 candidates (m·ªü r·ªông)
            for ticket, score in scored_candidates[:500]:
                if ticket in selected:
                    continue
                
                # ƒê·∫øm s·ªë c·∫∑p M·ªöI m√† ticket n√†y th√™m v√†o
                new_pairs = 0
                for pair in combinations(ticket, 2):
                    if pair not in covered_pairs:
                        new_pairs += 1
                
                # ƒê·∫øm s·ªë M·ªöI
                new_nums = len(set(ticket) - covered_numbers)
                
                # Value = score * (1 + coverage_bonus)
                coverage_bonus = new_pairs / 15 + new_nums / 6
                value = score * (1 + 0.4 * coverage_bonus)
                
                if value > best_value:
                    best_value = value
                    best_ticket = ticket
            
            if best_ticket is not None:
                selected.append(best_ticket)
                for pair in combinations(best_ticket, 2):
                    covered_pairs.add(pair)
                covered_numbers.update(best_ticket)
            else:
                for ticket, score in scored_candidates:
                    if ticket not in selected:
                        selected.append(ticket)
                        break
        
        return selected


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 4. MAIN ENTRY POINT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def run_ultra_prediction(product_type: str = "power_645", use_ai: bool = True) -> Tuple[str, List[List[int]]]:
    """
    Ch·∫°y d·ª± ƒëo√°n ULTRA v√† tr·∫£ v·ªÅ (report_string, tickets_list).
    
    Args:
        product_type: "power_645" ho·∫∑c "power_655"
        use_ai: True ƒë·ªÉ d√πng Deep Learning ensemble, False ch·ªâ d√πng th·ªëng k√™
    
    Returns:
        (report: str, tickets: List[List[int]])
    """
    import io
    from contextlib import redirect_stdout
    
    max_num = 55 if "655" in product_type else 45
    filename = "power655.jsonl" if "655" in product_type else "power645.jsonl"
    prod_name = "POWER 6/55" if "655" in product_type else "MEGA 6/45"
    
    # Locate data file
    base_dir = os.path.dirname(os.path.abspath(__file__))
    paths_to_try = [
        os.path.join(os.getcwd(), 'data', filename),
        os.path.join(base_dir, '..', '..', '..', 'data', filename),
        os.path.join(base_dir, 'data', filename)
    ]
    
    target_file = None
    for p in paths_to_try:
        if os.path.exists(p):
            target_file = p
            break
    
    f = io.StringIO()
    tickets = []
    
    with redirect_stdout(f):
        if not target_file:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {filename}")
            return f.getvalue(), []
        
        print("=" * 60)
        print(f"üèÜ ULTRA PREDICTOR v2.0 - {prod_name}")
        print(f"üìÖ Th·ªùi gian: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        print("=" * 60)
        
        # 1. Load data
        print(f"\nüìÇ Loading data t·ª´ {filename}...")
        draws = []
        with open(target_file, 'r', encoding='utf-8') as file:
            for line in file:
                try:
                    data = json.loads(line)
                    if 'result' in data:
                        nums = sorted([int(n) for n in data['result']])[:6]
                        draws.append(nums)
                except Exception:
                    continue
        
        # Sort by draw order (file is already ordered)
        print(f"   ‚úÖ Loaded {len(draws)} k·ª≥ quay")
        
        # Show latest 5 draws
        print(f"\n   üìã 5 k·ª≥ g·∫ßn nh·∫•t:")
        for i, draw in enumerate(draws[-5:], 1):
            print(f"      {i}. {draw} (T·ªïng: {sum(draw)})")
        
        # 2. Number Scoring (Statistical Analysis)
        print("\n" + "üìä" * 30)
        print("PH√ÇN T√çCH TH·ªêNG K√ä ƒêA T√çN HI·ªÜU")
        print("üìä" * 30)
        
        scorer = NumberScorer(draws, max_num)
        number_scores = scorer.compute_all_signals()
        pair_matrix = scorer.get_pair_matrix()
        
        # Show top 20 numbers by statistical score
        sorted_by_stat = sorted(number_scores.items(), key=lambda x: x[1], reverse=True)
        print(f"\n   üîù TOP 20 s·ªë c√≥ ƒëi·ªÉm th·ªëng k√™ cao nh·∫•t:")
        for num, score in sorted_by_stat[:20]:
            bar = "‚ñà" * int(score * 25)
            print(f"      S·ªë {num:2d}: {score:.3f} {bar}")
        
        # 3. AI Ensemble (if enabled)
        if use_ai:
            ensemble = DeepEnsemble(draws, max_num)
            ai_probs = ensemble.get_ensemble_probabilities()
        else:
            print("\n   ‚ö° Ch·∫ø ƒë·ªô nhanh: Ch·ªâ d√πng th·ªëng k√™ (kh√¥ng AI)")
            ai_probs = np.array([number_scores.get(i+1, 0) for i in range(max_num)])
        
        # 4. Ticket Optimization
        optimizer = TicketOptimizer(max_num, draws)
        tickets = optimizer.generate_optimal_tickets(
            number_scores, ai_probs, pair_matrix, count=18
        )
        
        # 5. Summary
        print("\n" + "=" * 60)
        print("üéØ K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN HYBRID v4.0 (Ultra + RE)")
        print("=" * 60)
        
        for i, ticket in enumerate(tickets, 1):
            print(f"   V√© {i:2d}: {' '.join([f'{n:02d}' for n in ticket])} (T·ªïng: {sum(ticket)})")
        
        print(f"\nüí° L∆ØU √ù:")
        print(f"   - Hybrid: {len(set(n for t in tickets for n in t))} s·ªë ph·ªß s√≥ng")
        print(f"   - S·ª≠ d·ª•ng {3 if use_ai else 0} m√¥ h√¨nh AI + 8 t√≠n hi·ªáu th·ªëng k√™")
        cs = 8 if max_num <= 45 else 9
        es = 14 if max_num <= 45 else 16
        print(f"   - Chi·∫øn l∆∞·ª£c: Core({cs}) + Extended({es}) + RE-Random(ALL)")
        print(f"   - RE-Random: sampling t·ª´ T·∫§T C·∫¢ {max_num} s·ªë v·ªõi tr·ªçng s·ªë th√¥ng minh")
        print(f"   - T·ªëi ∆∞u h√≥a coverage gi·ªØa {len(tickets)} v√©")
        print("=" * 60)
    
    return f.getvalue(), tickets


# Direct run
if __name__ == "__main__":
    import sys
    product = "power_645"
    if len(sys.argv) > 1:
        product = sys.argv[1]
    
    report, tickets = run_ultra_prediction(product, use_ai=True)
    print(report)
